# -*- coding: utf-8 -*-
"""Crop_Recommendation_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kt_RFz9l10Ih7Stb7NtlatpWRhyKm7cJ
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score
from google.colab import drive
drive.mount('/content/drive')

dataset = pd.read_csv('/content/drive/MyDrive/contents/normalized_dataset.csv')

x_val = dataset[['Normalized_Nitrogen', 'Normalized_phosphorus', 'Normalized_potassium', 'Normalized_temperature', 'Normalized_humidity', 'Normalized_pH', 'Normalized_rainfall']].values
y_val = dataset[['label']]
y_val = y_val.values.ravel()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_val, y_val, test_size=0.2, random_state=0)

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix

model = GaussianNB()
model.fit(x_train, y_train)

y_pred = model.predict(x_test)

accuracy_NB = accuracy_score(y_test, y_pred)
print("Accuracy for naive bayes:", accuracy_NB)

precision_NB = precision_score(y_test, y_pred, average='weighted') # Use 'weighted' average for multiclass
recall_NB = recall_score(y_test, y_pred, average='weighted') # Use 'weighted' average for multiclass
print("Precision for naive bayes:", precision_NB)
print("Recall for naive bayes:", recall_NB)

import seaborn as sns
import matplotlib.pyplot as plt
conf_matrix_NB = confusion_matrix(y_test, y_pred)
print("Confusion Matrix for naive bayes:")
print(conf_matrix_NB)

f, ax = plt.subplots(figsize=(15,10))
sns.heatmap(conf_matrix_NB, annot=True, linewidth=0.5, fmt=".0f",cmap='viridis', ax = ax)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Predicted vs actual')
plt.show()

cross_val_score_NB = cross_val_score(model, x_val, y_val, cv=5)

print("Cross-Validation Scores for naive bayes:", cross_val_score_NB)
print("Mean Cross-Validation Score for naive bayes:", cross_val_score_NB.mean())

from sklearn.svm import SVC
from sklearn import metrics

from sklearn.metrics import classification_report

model = SVC(gamma='auto')

model.fit(x_train,y_train)

y_pred = model.predict(x_test)

accuracy_SVM = accuracy_score(y_test, y_pred)

print("SVM's Accuracy is: ", accuracy_SVM)

precision_SVM = precision_score(y_test, y_pred, average='macro') # Use 'weighted' average for multiclass
recall_SVM = recall_score(y_test, y_pred, average='macro') # Use 'weighted' average for multiclass
print("Precision for svm:", precision_SVM)
print("Recall for svm:", recall_SVM)



import seaborn as sns
import matplotlib.pyplot as plt
conf_matrix_SVM = confusion_matrix(y_test, y_pred)
print("Confusion Matrix for svm:")
print(conf_matrix_SVM)

f, ax = plt.subplots(figsize=(15,10))
sns.heatmap(conf_matrix_SVM, annot=True, linewidth=0.5, fmt=".0f",cmap='viridis', ax = ax)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Predicted vs actual')
plt.show()

cross_val_score_SVM = cross_val_score(model, x_val, y_val, cv=5)

print("Cross-Validation Scores for svm:", cross_val_score_SVM)
print("Mean Cross-Validation Score for svm:", cross_val_score_SVM.mean())

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=20, random_state=0)
model.fit(x_train,y_train)
y_pred = model.predict(x_test)
accuracy_RF = accuracy_score(y_test, y_pred)
print("Random Forest's Accuracy is: ", accuracy_RF)

precision_RF = precision_score(y_test, y_pred, average='macro') # Use 'weighted' average for multiclass
recall_RF = recall_score(y_test, y_pred, average='macro') # Use 'weighted' average for multiclass
print("Precision for Random Forest:", precision_RF)
print("Recall for Random Forest:", recall_RF)

import seaborn as sns
import matplotlib.pyplot as plt
conf_matrix_RF = confusion_matrix(y_test, y_pred)
print("Confusion Matrix for Random Forest:")
print(conf_matrix_RF)

f, ax = plt.subplots(figsize=(15,10))
sns.heatmap(conf_matrix_RF, annot=True, linewidth=0.5, fmt=".0f",cmap='viridis', ax = ax)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Predicted vs actual')
plt.show()

cross_val_score_RF = cross_val_score(model, x_val, y_val, cv=5)

print("Cross-Validation Scores for random forest:", cross_val_score_RF)
print("Mean Cross-Validation Score for random forest:", cross_val_score_RF.mean())

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(random_state=2)

model.fit(x_train,y_train)

y_pred = model.predict(x_test)

accuracy_LR = accuracy_score(y_test, y_pred)


print("Logistic Regression's Accuracy is: ", accuracy_LR)





precision_LR = precision_score(y_test, y_pred, average='macro') # Use 'weighted' average for multiclass
recall_LR = recall_score(y_test, y_pred, average='macro') # Use 'weighted' average for multiclass
print("Precision for logistic regression:", precision_LR)
print("Recall for logistic regression:", recall_LR)

import seaborn as sns
import matplotlib.pyplot as plt
conf_matrix_LR = confusion_matrix(y_test, y_pred)
print("Confusion Matrix for logistic regression:")
print(conf_matrix_LR)

f, ax = plt.subplots(figsize=(15,10))
sns.heatmap(conf_matrix_LR, annot=True, linewidth=0.5, fmt=".0f",cmap='viridis', ax = ax)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Predicted vs actual')
plt.show()

cross_val_score_LR = cross_val_score(model, x_val, y_val, cv=5)

print("Cross-Validation Scores for logistic regression:", cross_val_score_LR)
print("Mean Cross-Validation Score for logistic regression:", cross_val_score_LR.mean())

# KNN algorithm use

import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier

# Define range of n_neighbors values to try
neighbors = range(1, 21)  # Try n_neighbors from 1 to 20

accuracies = []
precisions = []
recalls = []

for n in neighbors:
    # Train the KNN model
    model = KNeighborsClassifier(n_neighbors=n)
    model.fit(x_train, y_train)

    # Make predictions
    y_pred = model.predict(x_test)

    # Compute evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro')  # Calculate precision for each label and take the average
    recall = recall_score(y_test, y_pred, average='macro')  # Calculate recall for each label and take the average

    # Append metrics to lists
    accuracies.append(accuracy)
    precisions.append(precision)
    recalls.append(recall)

plt.figure(figsize=(10, 6))
plt.plot(neighbors, accuracies, label='Accuracy')
plt.plot(neighbors, precisions, label='Precision')
plt.plot(neighbors, recalls, label='Recall')
plt.xlabel('Number of Neighbors (n_neighbors)')
plt.ylabel('Score')
plt.title('KNN Performance for Different Number of Neighbors')
plt.xticks(neighbors)
plt.legend()
plt.grid(True)
plt.show()

cross_val_score_KNN = cross_val_score(model, x_val, y_val, cv=5)

print("Cross-Validation Scores for KNN:", cross_val_score_KNN)
print("Mean Cross-Validation Scores for KNN:", cross_val_score_KNN.mean())

import seaborn as sns
import matplotlib.pyplot as plt
conf_matrix_KNN = confusion_matrix(y_test, y_pred)
print("Confusion Matrix for KNN:")
print(conf_matrix_KNN)

f, ax = plt.subplots(figsize=(15,10))
sns.heatmap(conf_matrix_KNN, annot=True, linewidth=0.5, fmt=".0f",cmap='viridis', ax = ax)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Predicted vs actual')
plt.show()

from sklearn.tree import DecisionTreeClassifier, plot_tree

label_values = np.unique(y_val)
accuracies = np.zeros(len(label_values))
precisions = np.zeros(len(label_values))
recalls = np.zeros(len(label_values))

# Initialize the Decision Tree classifier
model = DecisionTreeClassifier(random_state=0)

model.fit(x_train, y_train)

y_pred = model.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')

# Append metrics to lists
accuracies[:] = accuracy
precisions[:] = precision
recalls[:] = recall

plt.figure(figsize=(25, 6))
plt.plot(label_values, accuracies, label='Accuracy')
plt.plot(label_values, precisions, label='Precision')
plt.plot(label_values, recalls, label='Recall')
plt.xlabel('Label Value')
plt.ylabel('Score')
plt.title('Decision Tree Performance for Different Label Values')
plt.xticks(label_values)
plt.legend()
plt.grid(True)
plt.show()

# Step 5: Visualize the Decision Tree
plt.figure(figsize=(100, 80))
plot_tree(model, filled=True, class_names=list(model.classes_))  # Convert classes_ array to list
plt.show()

cross_val_score_DT = cross_val_score(model, x_val, y_val, cv=5)

print("Cross-Validation Scores for Decision Tree:", cross_val_score_DT)
print("Mean Cross-Validation Score for Decision Tree:", cross_val_score_DT.mean())

import seaborn as sns
import matplotlib.pyplot as plt
conf_matrix_DT = confusion_matrix(y_test, y_pred)
print("Confusion Matrix for Decision Tree:")
print(conf_matrix_DT)

f, ax = plt.subplots(figsize=(15,10))
sns.heatmap(conf_matrix_DT, annot=True, linewidth=0.5, fmt=".0f",cmap='viridis', ax = ax)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Predicted vs actual')
plt.show()





from sklearn.metrics import f1_score
# svm_model = SVC(gamma='auto')
nb_model = GaussianNB()
knn_model = KNeighborsClassifier()
dt_model = DecisionTreeClassifier()
lr_model = LogisticRegression(random_state=2)
rf_model = RandomForestClassifier(n_estimators=20, random_state=0)


models = [nb_model, knn_model, dt_model, lr_model, rf_model]
model_names = ['Naive Bayes', 'KNN', 'Decision Tree', 'Logistic Regression', 'Random Forest']

accuracies = []
precisions = []
recalls = []
f1_scores = []

for model in models:
    # Perform cross-validation
    accuracy = np.mean(cross_val_score(model, x_val, y_val, cv=5, scoring='accuracy'))
    precision = np.mean(cross_val_score(model, x_val, y_val, cv=5, scoring='precision_macro'))
    recall = np.mean(cross_val_score(model, x_val, y_val, cv=5, scoring='recall_macro'))
    f1 = np.mean(cross_val_score(model, x_val, y_val, cv=5, scoring='f1_macro'))

    accuracies.append(accuracy)
    precisions.append(precision)
    recalls.append(recall)
    f1_scores.append(f1)

plt.figure(figsize=(10, 6))
plt.plot(model_names, accuracies, label='Accuracy')
plt.plot(model_names, precisions, label='Precision')
plt.plot(model_names, recalls, label='Recall')
plt.plot(model_names, f1_scores, label='F1 Score')
plt.xlabel('Models')
plt.ylabel('Score')
plt.title('Performance Metrics for Different Models')
plt.legend()
plt.grid(True)
plt.show()

x = np.arange(len(model_names))
width = 0.2  # Width of the bars

plt.figure(figsize=(10, 6))
plt.bar(x - 1.5*width, accuracies, width, label='Accuracy')
plt.bar(x - 0.5*width, precisions, width, label='Precision')
plt.bar(x + 0.5*width, recalls, width, label='Recall')
plt.bar(x + 1.5*width, f1_scores, width, label='F1 Score')

plt.xlabel('Models')
plt.ylabel('Score')
plt.title('Performance Metrics for Different Models')
plt.xticks(x, model_names)
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import make_scorer
# Initialize dictionary to store performance metrics
metrics_dict = {'Model': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': []}

# Define custom scorers for precision, recall, and f1
precision_scorer = make_scorer(precision_score, average='weighted')
recall_scorer = make_scorer(recall_score, average='macro')
f1_scorer = make_scorer(f1_score, average='macro')

# Loop through each model
for model, model_name in zip(models, model_names):
    # Perform cross-validation for each metric
    accuracy = cross_val_score(model, x_val, y_val, cv=5, scoring='accuracy').mean()
    precision = cross_val_score(model, x_val, y_val, cv=5, scoring=precision_scorer).mean()
    recall = cross_val_score(model, x_val, y_val, cv=5, scoring=recall_scorer).mean()
    f1 = cross_val_score(model, x_val, y_val, cv=5, scoring=f1_scorer).mean()

    # Store metrics in the dictionary
    metrics_dict['Model'].append(model_name)
    metrics_dict['Accuracy'].append(accuracy)
    metrics_dict['Precision'].append(precision)
    metrics_dict['Recall'].append(recall)
    metrics_dict['F1 Score'].append(f1)

metrics_df = pd.DataFrame(metrics_dict)

# Display the DataFrame
print("Performance Metrics for Different Models:")
print(metrics_df)

